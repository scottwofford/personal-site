<!DOCTYPE html>
<html lang="es">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Filosofía Personal - Scott Wofford</title>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inconsolata:wght@400;600&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="/styles.css">
</head>
<body>
  <div class="section-nav">
    <a href="/es/">← inicio</a> / <a href="/es/philosophy/" class="section-name">filosofía y epistémica</a>
    <div class="siblings">
      <a href="/es/philosophy/personal-philosophy.html" class="current">Filosofía Personal</a>
      <a href="/es/philosophy/wrong-on-internet.html">Error en Internet</a>
    </div>
  </div>
  <div style="text-align:right;margin-bottom:1rem"><a href="/philosophy/personal-philosophy.html" style="color:#555;font-size:0.85rem">EN</a></div>

  <h1>Filosofía Personal</h1>
  <p class="subtitle">Un trabajo en progreso. Sujeto a revisión.</p>

  <hr>

  <h2>Marco fundamental</h2>
  <p>Hasta donde puedo determinar, soy una especie de consecuencialista con modificaciones. El razonamiento de valor esperado se siente fundamental para las decisiones importantes, pero noto que opero dentro de restricciones y umbrales que el utilitarismo puro no requeriría. Todavía estoy descubriendo por qué.</p>

  <h3>Las modificaciones</h3>

  <p><strong>Requisitos de integridad epistémica.</strong> Me encuentro desconfiando de la confianza que supera el razonamiento subyacente. Cuando alguien da una estimación estrecha de p(doom), mi instinto no es "estás equivocado" sino "no estoy seguro de que la confianza que implica tu respuesta esté justificada por el trabajo que hiciste para llegar a ella." Prefiero decir "5-25%, dependiendo de supuestos que no puedo precisar" que aparentar falsa precisión. Para mí esto se siente como algo más que una preferencia metodológica, aunque no estoy seguro.</p>

  <p><strong>Compromisos de agencia.</strong> Creo que los individuos deberían poder especificar cómo se comportan los sistemas de IA en su nombre, incluso si sus especificaciones no son globalmente óptimas. <a href="https://luthienresearch.org">Luthien</a> no se construyó sobre "sabemos lo que la IA debería hacer", sino sobre "los usuarios deberían poder definir sus propias restricciones y verificar que se cumplan." Esto parece una preferencia por preservar la autonomía humana superpuesta al consecuencialismo.</p>

  <p><strong>Efectos de umbral para las demandas morales.</strong> Sobre el bienestar animal, por ejemplo: no tengo suficiente confianza en los pesos morales como para reestructurar mi vida en torno a ellos. Un consecuencialista puro podría decir que si el cálculo de valor esperado favorece el veganismo, hazlo sin importar la confianza. Yo parezco aplicar un umbral: las demandas morales necesitan superar una barra epistémica antes de obligarme. No estoy seguro de si esto es principista o simplemente conveniente.</p>

  <p><strong>Paternalismo libertario como filosofía de diseño.</strong> En lugar de intentar resolver "¿qué debería hacer la IA?" globalmente, me inclino hacia crear buenos valores predeterminados para modos de fallo predecibles mientras se preserva la capacidad del usuario de anularlos. Arquitectura de elección sobre mandatos. Empujón sobre prohibición. Encuentro esto más abordable, aunque otros podrían razonablemente discrepar.</p>

  <hr>

  <h2>El hilo de Wittgenstein</h2>
  <p>Me atrajo Wittgenstein en la filosofía universitaria. Mirando hacia atrás, su obra tardía parece corresponderse con mi pensamiento más de lo que inicialmente reconocí, aunque puede que esté buscando patrones con demasiado entusiasmo.</p>

  <p><strong>El significado como uso.</strong> Wittgenstein argumentó que el significado no se encuentra en definiciones abstractas sino en la aplicación práctica: cómo funcionan las palabras en los "juegos de lenguaje" reales. Esto resuena con mi instinto de profundizar pero salir a la superficie para preguntar "¿y entonces qué?" No me interesa mucho aprender por aprender; quiero saber cómo se traducen los conceptos en la práctica.</p>

  <p><strong>Disolver problemas falsos.</strong> Wittgenstein creía que muchos problemas filosóficos no se resuelven sino que se disuelven: surgen del mal uso del lenguaje o de hacer preguntas mal formuladas. Me pregunto si el alineamiento es similar: "¿qué debería valorar la IA?" puede ser filosóficamente irresoluble tal como está planteado, pero "¿pueden los usuarios especificar restricciones y verificar su cumplimiento?" se siente más abordable.</p>

  <p><strong>Semejanza de familia sobre categorías rígidas.</strong> Wittgenstein rechazó la idea de que las categorías tienen definiciones esenciales. En su lugar, los miembros comparten similitudes superpuestas, como semejanzas de familia, sin un hilo común único. Esto coincide con mi sensación de que el mundo es más desordenado de lo que las categorías limpias sugieren.</p>

  <p><strong>Contra la falsa precisión.</strong> Wittgenstein criticó a los filósofos que buscaban más precisión de la que su tema permitía. Esto captura algo de mi incomodidad con rangos estrechos de p(doom). El teatro de la precisión se siente peor que la incertidumbre honesta.</p>

  <hr>

  <h2>Ejemplos concretos</h2>

  <p><strong>Elección de carrera:</strong> Dejé una carrera de 9 años como Principal PM en Amazon para cofundar una startup de seguridad de IA. La lógica de la decisión fue consecuencialista (mayor impacto esperado), pero trabajar directamente en el problema me importa, no solo financiarlo. Hay algo sobre la agencia y la contribución directa que mi marco valora más allá de la pura optimización de resultados.</p>

  <p><strong>P(doom) y preguntas de líneas temporales:</strong> Creo que la mejor pregunta es: ¿tu p(doom) está por encima del umbral que te haría trabajar en seguridad de IA a tiempo completo? Los humanos somos malos con los números pequeños: un p(doom) de 0.01% debería ser motivador si lo tomas en serio. Prefiero centrarme en eso que en la falsa precisión sobre rangos.</p>

  <p><strong>Agentes de IA que "mienten" y "hacen trampa":</strong> En la taxonomía de Luthien, categorizamos los fallos de IA usando lenguaje moral, no solo lenguaje de optimización. Un agente que dice una cosa y hace otra no solo está cometiendo errores: se siente como si estuviera violando algo. Confianza, normas de honestidad, el deber de ser lo que te presentas como. No estoy seguro de si este encuadre es filosóficamente defendible, pero es como pienso al respecto.</p>

  <hr>

  <h2>La síntesis</h2>
  <p>Si tuviera que resumir: parezco ser un consecuencialista que toma el valor esperado en serio para las grandes decisiones, pero opera con:</p>
  <ol>
    <li><strong>Requisitos de integridad epistémica</strong>: intenta no afirmar más confianza de la que tienes</li>
    <li><strong>Compromisos de agencia</strong>: los individuos pueden especificar sus propios valores</li>
    <li><strong>Efectos de umbral</strong>: las demandas morales inciertas no obligan completamente</li>
    <li><strong>Filosofía de diseño paternalista libertaria</strong>: buenos valores predeterminados, capacidad de anulación del usuario, transparencia</li>
  </ol>

  <p>Este marco parece internamente consistente y explica mis elecciones reales, al menos así se ve desde dentro. No es puramente nada, pero espero que sea coherente.</p>

  <hr>

  <h2>El "¿Y qué?" para mi trabajo</h2>
  <p><a href="https://luthienresearch.org">Luthien</a> no es solo (espero) de alto valor esperado. Me gustaría que encarnara estas ideas:</p>

  <p><strong>Humildad epistémica:</strong> No afirmamos saber qué debería valorar la IA. Lo que decimos es: estos son los modos de fallo predecibles, estos son los valores predeterminados que los capturan, así es como puedes personalizarlos.</p>

  <p><strong>Preservación de la agencia:</strong> Los usuarios especifican sus propias restricciones. El sistema es arquitectura de elección, no imposición de valores.</p>

  <p><strong>La honestidad como algo que importa:</strong> La "mentira" de la IA (inconsistencia entre la intención declarada y el comportamiento real) es un modo de fallo de primera clase. No estoy seguro de si esto es porque el engaño produce malos resultados o porque es específicamente incorrecto, probablemente ambos.</p>

  <p><strong>Paternalismo libertario:</strong> Buenos valores predeterminados, fácil anulación, transparencia sobre la arquitectura. Empujón sobre prohibición. Habilitar sobre imponer.</p>

</body>
</html>
