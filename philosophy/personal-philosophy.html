<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Personal Philosophy - Scott Wofford</title>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inconsolata:wght@400;600&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="/styles.css">
</head>
<body>
  <div class="section-nav">
    <a href="/">← home</a> / <a href="/philosophy/" class="section-name">philosophy & epistemics</a>
    <div class="siblings">
      <a href="/philosophy/personal-philosophy.html" class="current">Philosophy</a>
      <a href="/philosophy/wrong-on-internet.html">Wrong on Internet</a>
    </div>
  </div>

  <h1>Personal Philosophy</h1>
  <p class="subtitle">A work in progress. Subject to revision.</p>

  <hr>

  <h2>Core Framework</h2>
  <p>As best I can tell, I'm some kind of consequentialist with modifications. Expected value reasoning feels load-bearing for major decisions, but I notice I operate within constraints and thresholds that pure utilitarianism wouldn't require. I'm still figuring out why.</p>

  <h3>The Modifications</h3>

  <p><strong>Epistemic integrity requirements.</strong> I find myself distrusting confidence that outpaces the underlying reasoning. When someone gives a narrow p(doom) estimate, my instinct isn't "you're wrong" but "I'm not sure the confidence your answer implies is justified by the work you did to arrive at it." I'd rather say "5-25%, depending on assumptions I can't pin down" than perform false precision. This feels like more than methodological preference to me—though I'm not certain.</p>

  <p><strong>Agency commitments.</strong> I think individuals should be able to specify how AI systems behave on their behalf, even if their specifications aren't globally optimal. <a href="https://luthienresearch.org">Luthien</a> isn't built on "we know what AI should do"—it's built on "users should be able to define their own constraints and verify they're enforced." This seems like a preference for preserving human autonomy layered on consequentialism.</p>

  <p><strong>Threshold effects for moral demands.</strong> On animal welfare, for example: I'm not confident enough in the moral weights to restructure my life around them. A pure consequentialist might say if the EV calculation favors veganism, do it regardless of confidence. I seem to apply a threshold—moral demands need to clear an epistemic bar before they bind me. I'm not sure if this is principled or just convenient.</p>

  <p><strong>Libertarian paternalism as design philosophy.</strong> Rather than trying to solve "what should AI do?" globally, I'm drawn to creating good defaults for predictable failure modes while preserving user override. Choice architecture over mandates. Nudge over ban. I find this more tractable, though others might reasonably disagree.</p>

  <hr>

  <h2>The Wittgenstein Thread</h2>
  <p>I was drawn to Wittgenstein in college philosophy. Looking back, his later work seems to map onto my thinking more than I initially recognized—though I may be pattern-matching too eagerly.</p>

  <p><strong>Meaning as use.</strong> Wittgenstein argued that meaning isn't found in abstract definitions but in practical application—how words function in actual "language games." This resonates with my instinct to dive deep but surface to ask "so what?" I'm not that interested in learning for its own sake; I want to know how concepts cash out in practice.</p>

  <p><strong>Dissolving false problems.</strong> Wittgenstein believed many philosophical problems aren't solved but dissolved—they arise from misusing language or asking malformed questions. I wonder if alignment is similar: "what should AI value?" may be philosophically unsolvable as posed, but "can users specify constraints and verify enforcement?" feels more tractable.</p>

  <p><strong>Family resemblance over rigid categories.</strong> Wittgenstein rejected the idea that categories have essential definitions. Instead, members share overlapping similarities—like family resemblances—without a single common thread. This matches my sense that the world is messier than clean categories suggest.</p>

  <p><strong>Against false precision.</strong> Wittgenstein critiqued philosophers who sought more precision than their subject matter allowed. This captures something about my discomfort with narrow p(doom) ranges. Precision theater feels worse than honest uncertainty.</p>

  <hr>

  <h2>Concrete Examples</h2>

  <p><strong>Career choice:</strong> I left a 9-year Principal PM career at Amazon to co-found an AI safety startup. The decision logic was consequentialist (highest expected impact), but working directly on the problem matters to me, not just funding it. There's something about agency and direct contribution that my framework values beyond pure outcome optimization.</p>

  <p><strong>P(doom) and timeline questions:</strong> I think the better question to ask is: is your p(doom) above the threshold that would cause you to work on AI safety full time? Humans are bad at small numbers—a p(doom) of 0.01% should still be motivating if you take it seriously. I'd rather focus on that than false precision about ranges.</p>

  <p><strong>AI agents "lying" and "cheating":</strong> In Luthien's taxonomy, we categorize AI failures using moral language, not just optimization language. An agent that says one thing and does another isn't just making errors—it feels like it's violating something. Trust, honesty norms, the duty to be what you present yourself as. I'm not sure if this framing is philosophically defensible, but it's how I think about it.</p>

  <hr>

  <h2>The Synthesis</h2>
  <p>If I had to summarize: I seem to be a consequentialist who takes expected value seriously for big decisions, but operates with:</p>
  <ol>
    <li><strong>Epistemic integrity requirements</strong>—try not to claim more confidence than you have</li>
    <li><strong>Agency commitments</strong>—individuals get to specify their own values</li>
    <li><strong>Threshold effects</strong>—uncertain moral demands don't fully bind</li>
    <li><strong>Libertarian paternalist design philosophy</strong>—good defaults, user override, transparency</li>
  </ol>

  <p>This framework seems internally consistent and explains my actual choices—at least, that's how it looks from the inside. It's not pure anything, but hopefully coherent.</p>

  <hr>

  <h2>The "So What?" for My Work</h2>
  <p><a href="https://luthienresearch.org">Luthien</a> isn't just (I hope) high-EV. I'd like it to embody these ideas:</p>

  <p><strong>Epistemic humility:</strong> We're not claiming to know what AI should value. We're saying: here are predictable failure modes, here are defaults that catch them, here's how to customize.</p>

  <p><strong>Agency preservation:</strong> Users specify their own constraints. The system is choice architecture, not value imposition.</p>

  <p><strong>Honesty as something that matters:</strong> AI "lying" (inconsistency between stated intent and actual behavior) is a first-class failure mode. I'm not certain if this is because deception produces bad outcomes or because it's specifically wrong—probably both.</p>

  <p><strong>Libertarian paternalism:</strong> Good defaults, easy override, transparency about the architecture. Nudge over ban. Enable over mandate.</p>

</body>
</html>
