<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Personal Philosophy - Scott Wofford</title>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inconsolata:wght@400;600&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="/styles.css">
</head>
<body>
  <div class="section-nav">
    <a href="/">← home</a> / <a href="/philosophy/" class="section-name">philosophy & epistemics</a>
    <div class="siblings">
      <a href="/philosophy/personal-philosophy.html" class="current">Philosophy</a>
      <a href="/philosophy/x-wrong-on-internet.html">X</a>
      <a href="/philosophy/wikipedia-wrong-on-internet.html">Wikipedia</a>
    </div>
  </div>

  <h1>Personal Philosophy</h1>
  <p class="subtitle">Consequentialism with modifications.</p>

  <hr>

  <h2>Core Framework</h2>
  <p>I'm a consequentialist with significant modifications. Expected value reasoning is load-bearing for major decisions, but it operates within constraints and thresholds that pure utilitarianism doesn't require.</p>

  <h3>The Modifications</h3>

  <p><strong>Epistemic integrity requirements.</strong> I distrust confidence that outpaces the underlying reasoning. When someone gives a precise p(doom) estimate, my objection isn't "you're wrong" but "the confidence your answer implies isn't justified by the enumeration you did to arrive at it." I'd rather say "5-25%, depending on assumptions I can't pin down" than perform false precision. This isn't just methodological preference—it's a moral stance about intellectual honesty.</p>

  <p><strong>Agency commitments.</strong> I believe individuals should be able to specify how AI systems behave on their behalf, even if their specifications aren't globally optimal. <a href="https://luthienresearch.org">Luthien</a> isn't built on "we know what AI should do"—it's built on "users should be able to define their own constraints and verify they're enforced." This is a strong preference for preserving human autonomy layered on consequentialism.</p>

  <p><strong>Deontological constraints around honesty.</strong> I wouldn't lie to a funder even if it meaningfully increased Luthien's impact. I might embellish—selective emphasis, favorable framing, letting people draw inferences I won't explicitly state—but I won't assert things I believe to be false. The constraint is about my relationship to truth, not about outcomes.</p>

  <p><strong>Threshold effects for moral demands.</strong> On animal welfare: "I'm not confident enough in the moral weights to restructure my life around them." A pure consequentialist would say if the EV calculation favors veganism, do it regardless of confidence. I apply a threshold—moral demands need to clear an epistemic bar before they bind me.</p>

  <p><strong>Libertarian paternalism as design philosophy.</strong> Rather than solving "what should AI do?" globally, create good defaults for predictable failure modes while preserving user override. The control system is choice architecture. Don't ban—nudge. Don't mandate—make good outcomes the path of least resistance.</p>

  <hr>

  <h2>The Wittgenstein Thread</h2>
  <p>I was drawn to Wittgenstein in college philosophy, and his later work maps onto my thinking more than I initially recognized.</p>

  <p><strong>Meaning as use.</strong> Wittgenstein argued that meaning isn't found in abstract definitions but in practical application—how words function in actual "language games." This is my instinct: diving deep but surfacing to ask "so what?" I'm not interested in learning for its own sake; I want to know how concepts cash out in practice.</p>

  <p><strong>Dissolving false problems.</strong> Wittgenstein believed many philosophical problems aren't solved but dissolved—they arise from misusing language or asking malformed questions. My view on alignment is similar: "what should AI value?" may be philosophically unsolvable as posed, but "can users specify constraints and verify enforcement?" is tractable.</p>

  <p><strong>Family resemblance over rigid categories.</strong> Wittgenstein rejected the idea that categories have essential definitions. Instead, members share overlapping similarities—like family resemblances—without a single common thread. The world is messier than clean categories suggest.</p>

  <p><strong>Against false precision.</strong> Wittgenstein critiqued philosophers who sought more precision than their subject matter allowed. This is exactly my objection to p(doom) point estimates. Precision theater is worse than honest uncertainty.</p>

  <hr>

  <h2>Concrete Examples</h2>

  <p><strong>Career choice:</strong> I left a 9-year Principal PM career at Amazon to co-found an AI safety nonprofit. The decision logic was consequentialist (highest expected impact), but working directly on the problem matters to me, not just funding it. There's something about agency and direct contribution that my framework values beyond pure outcome optimization.</p>

  <p><strong>P(doom) and timeline questions:</strong> When asked for point estimates, I push back on the question structure. "P(doom) conflates misalignment, misuse, structural risks, accidents—each with different threat models, different interventions, different probability estimates." I'd rather give useful uncertainty than false precision.</p>

  <p><strong>AI agents "lying" and "cheating":</strong> In Luthien's taxonomy, I categorize AI failures using moral language, not just optimization language. An agent that says one thing and does another isn't just making errors—it's violating something. Trust, honesty norms, the duty to be what you present yourself as.</p>

  <hr>

  <h2>The Synthesis</h2>
  <p>I'm a consequentialist who takes expected value seriously for big decisions, but I operate with:</p>
  <ol>
    <li><strong>Epistemic integrity requirements</strong>—don't claim more confidence than you have</li>
    <li><strong>Agency commitments</strong>—individuals get to specify their own values</li>
    <li><strong>Deontological constraints around honesty</strong>—no lying, embellishment is a gray zone</li>
    <li><strong>Threshold effects</strong>—uncertain moral demands don't fully bind</li>
    <li><strong>Libertarian paternalist design philosophy</strong>—good defaults, user override, transparency</li>
  </ol>

  <p>This framework is internally consistent and explains my actual choices. It's not pure anything, but coherent.</p>

  <hr>

  <h2>The "So What?" for My Work</h2>
  <p><a href="https://luthienresearch.org">Luthien</a> isn't just high-EV. It embodies my ethics:</p>

  <p><strong>Epistemic humility:</strong> I'm not claiming to know what AI should value. I'm saying: here are predictable failure modes, here are defaults that catch them, here's how to customize.</p>

  <p><strong>Agency preservation:</strong> Users specify their own constraints. The system is choice architecture, not value imposition.</p>

  <p><strong>Honesty as terminal value:</strong> AI "lying" (inconsistency between stated intent and actual behavior) is a first-class failure mode, not because it produces bad outcomes, but because deception is specifically wrong.</p>

  <p><strong>Libertarian paternalism:</strong> Good defaults, easy override, transparency about the architecture. Don't ban—nudge. Don't mandate—enable.</p>

  <p><em>The philosophical problem of alignment is unsolvable. The engineering problem of control is tractable. I'm building the bridge.</em></p>

</body>
</html>
