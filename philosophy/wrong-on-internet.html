<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Someone is Wrong on the Internet - Scott Wofford</title>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inconsolata:wght@400;600&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="/styles.css">
  <style>
    .meme-container {
      text-align: center;
      margin: 1.5rem 0;
    }
    .meme-container img {
      max-width: 100%;
      border-radius: 4px;
    }
    .meme-caption {
      font-size: 0.85rem;
      color: #666;
      margin-top: 0.5rem;
    }
  </style>
</head>
<body>
  <div class="section-nav">
    <a href="/">← home</a> / <a href="/philosophy/" class="section-name">philosophy & epistemics</a>
    <div class="siblings">
      <a href="/philosophy/personal-philosophy.html">Philosophy</a>
      <a href="/philosophy/wrong-on-internet.html" class="current">Wrong on Internet</a>
    </div>
  </div>

  <h1>Someone is Wrong on the Internet</h1>
  <p class="subtitle">Community Notes, Wikipedia, and the urge to fix things.</p>

  <hr>

  <div class="meme-container">
    <img src="https://imgs.xkcd.com/comics/duty_calls.png" alt="xkcd 386: Duty Calls - 'I can't go to bed yet, someone is WRONG on the internet'">
    <p class="meme-caption">xkcd #386: "Duty Calls" — <a href="https://xkcd.com/386/">source</a></p>
  </div>

  <hr>

  <h2>Why I Do This</h2>
  <p>There's something deeply satisfying about fixing wrong information where people actually look for answers. Wikipedia is read by billions. Community Notes reach millions on X. When I see something incorrect—especially on topics I know well—I feel a pull to fix it.</p>

  <p>This isn't about being right. It's about the information ecosystem being better. Every corrected article, every helpful Community Note, is a small improvement to the shared knowledge base we all rely on.</p>

  <hr>

  <h2>Community Notes on X</h2>
  <p>I'm not much of a poster. My main contribution is <strong>Community Notes</strong>—the crowdsourced fact-checking system that adds context to misleading posts.</p>

  <p>Interestingly, most of the notes I write and rate are in Spanish. There seems to be a supply gap: not enough Community Notes contributors who speak Spanish, relative to the amount of Spanish-language misinformation that needs context.</p>

  <p>Community Notes has a clever design: notes only appear if they get agreement from people across the political spectrum. This "bridging" algorithm means notes that do appear tend to be genuinely informative rather than partisan dunks. The Spanish-language gap means misinformation can spread longer before getting context. Every note I write or rate helps close that gap a little.</p>

  <hr>

  <h2>Wikipedia Editing</h2>
  <p>I've been editing Wikipedia since college. It started with fixing small errors on topics I cared about and evolved into more substantial contributions. The process is meditative: find a claim, verify it, improve it, cite it properly.</p>

  <p>What I love about Wikipedia:</p>
  <ul>
    <li><strong>Permanence.</strong> Edits persist. A fix I make today helps readers for years.</li>
    <li><strong>Neutrality norms.</strong> The community enforces balanced coverage, even on contentious topics.</li>
    <li><strong>Verification culture.</strong> Claims need sources. This is how epistemics should work.</li>
  </ul>

  <hr>

  <h2>Automating "Somebody is Wrong"</h2>
  <p>Here's the thing: this doesn't scale. One person can only fix so many errors. The internet generates wrong information faster than humans can correct it.</p>

  <p>This is part of why I'm working on <a href="https://luthienresearch.org">Luthien</a>. AI agents are increasingly writing content, making claims, taking actions. The question isn't whether AI will shape information—it's whether we can verify that AI-generated content meets standards for accuracy and honesty.</p>

  <p>What if we could:</p>
  <ul>
    <li>Automatically flag claims that need verification?</li>
    <li>Surface discrepancies between what an AI says and what it does?</li>
    <li>Help humans review AI outputs more efficiently?</li>
  </ul>

  <p>The same instinct that makes me edit Wikipedia at midnight—<em>someone is wrong on the internet and I must fix it</em>—now drives my work on AI control. The scale is different. The impulse is the same.</p>

  <hr>

  <p><em>"I can't go to bed yet. Someone is wrong on the internet."</em></p>
  <p><em>¿Alguien está equivocado en el internet? Déjame ayudar.</em></p>

</body>
</html>
