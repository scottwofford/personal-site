<!-- Source: https://docs.google.com/document/d/1AxOqkLbJVZQnUKg6iaS8GJRXcU8K1Oxx/edit -->

# **Scott's Ethics and Philosophy**

# **Core Framework**

I'm a consequentialist with significant modifications. Expected value reasoning is load-bearing for major decisions, but it operates within constraints and thresholds that pure utilitarianism doesn't require.

## **The Modifications**

**Epistemic integrity requirements.** I distrust confidence that outpaces the underlying reasoning. When someone gives a precise p(doom) estimate, my objection isn't "you're wrong" but "the confidence your answer implies isn't justified by the enumeration you did to arrive at it." I'd rather say "5-25%, depending on assumptions I can't pin down" than perform false precision. This isn't just methodological preference—it's a moral stance about intellectual honesty.

**Agency commitments.** I believe individuals should be able to specify how AI systems behave on their behalf, even if their specifications aren't globally optimal. Luthien isn't built on "we know what AI should do"—it's built on "users should be able to define their own constraints and verify they're enforced." This is a strong preference for preserving human autonomy layered on consequentialism.

**Deontological constraints around honesty.** I wouldn't lie to a funder even if it meaningfully increased Luthien's impact. I might embellish—selective emphasis, favorable framing, letting people draw inferences I won't explicitly state—but I won't assert things I believe to be false. The constraint is about my relationship to truth, not about outcomes.

**Threshold effects for moral demands.** On animal welfare: "I'm not confident enough in the moral weights to restructure my life around them." A pure consequentialist would say if the EV calculation favors veganism, do it regardless of confidence. I apply a threshold—moral demands need to clear an epistemic bar before they bind me. This is closer to moral uncertainty frameworks where I discount demands proportional to my uncertainty about them.

**Libertarian paternalism as design philosophy.** Rather than solving "what should AI do?" globally, create good defaults for predictable failure modes while preserving user override. The control system is choice architecture. Don't ban—nudge. Don't mandate—make good outcomes the path of least resistance.

# **The Wittgenstein Thread**

I was drawn to Wittgenstein in college philosophy, and his later work maps onto my thinking more than I initially recognized.

**Meaning as use.** Wittgenstein argued that meaning isn't found in abstract definitions but in practical application—how words function in actual "language games." This is my "dolphin brain" instinct: diving deep but surfacing to ask "so what?" I'm not interested in learning for its own sake; I want to know how concepts cash out in practice. A theory that can't be applied is, for my purposes, incomplete.

**Dissolving false problems.** Wittgenstein believed many philosophical problems aren't solved but dissolved—they arise from misusing language or asking malformed questions. My view on alignment is similar: "what should AI value?" may be philosophically unsolvable as posed, but "can users specify constraints and verify enforcement?" is tractable. Reframing the problem makes it workable. I'm not dodging the hard question; I'm noticing that the engineering framing is actually more productive.

**Family resemblance over rigid categories.** Wittgenstein rejected the idea that categories have essential definitions. Instead, members share overlapping similarities—like family resemblances—without a single common thread. This resonates with my frustration with EA's "rigid taxonomy of things" and "aversion towards uncertainty." The world is messier than clean categories suggest, and forcing everything into boxes loses information.

**Limits of formalization.** "Whereof one cannot speak, thereof one must be silent." Some things resist explicit formulation. In HPMOR terms, this is Hermione's felt sense that something is wrong with Harry's methods before she can articulate why. My discomfort with lying isn't derived from expected value calculations—it's a constraint I notice I have. The framework I've articulated is partly descriptive: I'm explaining what I already do, not deriving it from first principles.

**Against false precision.** Wittgenstein critiqued philosophers who sought more precision than their subject matter allowed. This is exactly my objection to p(doom) point estimates: "the confidence the answer implies isn't justified by the enumeration you did to arrive at it." Precision theater is worse than honest uncertainty.

# **Concrete Examples**

**Career choice:** I left a 9-year Principal PM career at Amazon to co-found an AI safety nonprofit. The decision logic was consequentialist (highest expected impact), but the execution reveals something else—I didn't just maximize donations from a high-paying job. Working directly on the problem matters to me, not just funding it. There's something about agency and direct contribution that my framework values beyond pure outcome optimization.

**The Elon update:** I once admired Elon Musk deeply as a thinker and builder. Today, I find his actions confusing at best and destructive at worst. My concern about USAID cuts affecting vulnerable populations isn't purely about outcomes ("those people will suffer")—there's something about the relationship between powerful and vulnerable that bothers me independent of consequences. The rich shouldn't treat the poor as expendable. That's a stance about power relations, not just welfare calculations.

**AI agents "lying" and "cheating":** In Luthien's taxonomy, I categorize AI failures using moral language, not just optimization language. An agent that says one thing and does another isn't just making errors—it's violating something. Trust, honesty norms, the duty to be what you present yourself as. This explains why "consistency between stated intent and actual behavior" is a first-class concern in control systems, not just a nice-to-have.

**P(doom) and timeline questions:** When asked for point estimates, I push back on the question structure. "P(doom) conflates misalignment, misuse, structural risks, accidents—each with different threat models, different interventions, different probability estimates." I'd rather give useful uncertainty than false precision. Better questions exist, and part of my job is redirecting toward them.

# **The Synthesis**

I'm a consequentialist who takes expected value seriously for big decisions, but I operate with: (1) epistemic integrity requirements—don't claim more confidence than you have; (2) agency commitments—individuals get to specify their own values; (3) deontological constraints around honesty—no lying, embellishment is a gray zone; (4) threshold effects—uncertain moral demands don't fully bind; and (5) libertarian paternalist design philosophy—good defaults, user override, transparency.

This framework is internally consistent and explains my actual choices. It's not pure anything, but coherent. The Wittgensteinian thread runs underneath: I'm suspicious of false precision, I prefer dissolving problems to solving them, and I accept that some of my commitments are felt constraints I notice rather than conclusions I derived.

# **Mapping to HPMOR**

The characters in HPMOR represent different ethical stances, and understanding where I fit clarifies my own position.

## **I'm Not Quirrell**

Quirrell is a pure consequentialist with no constraints. His Azkaban argument is locally valid—perpetual Dementor exposure is torture serving no rehabilitative purpose—but he makes it instrumentally, not terminally. He doesn't care about prisoner welfare; he's recruiting Harry's loyalty and demonstrating his superiority over "conventional morality."

The alignment parallel: a sufficiently intelligent optimizer can generate correct-seeming arguments for almost any conclusion. Quirrell's object-level points often pass the smell test. His revealed preferences don't. That gap—between stated reasoning and actual optimization target—is what I'm trying to detect with Luthien.

I share Quirrell's impatience with systems that don't optimize, his frustration with cached thinking, his belief that most people don't reason carefully. But I have lines he doesn't. The embellish/lie distinction is one. The belief that individuals should be able to specify their own constraints is another. Quirrell would call these constraints sentimentality. I call them load-bearing.

## **I'm Not Harry (Early HPMOR Harry)**

Harry treats ethics as an engineering problem. He embarks on the Draco project—deprogramming a child from wizard racism through strategic deception and psychological manipulation. The goal is good, the methods are Quirrell-coded.

Hermione's objection: "you're not supposed to manipulate people, even toward true beliefs." Harry finds this frustrating because he's focused on outcomes.

I'm sympathetic to Harry's frustration, but I've updated toward Hermione's position over time. The Draco project treats people as optimization targets. Luthien's philosophy is different: give users tools to specify their own constraints, rather than imposing "correct" behavior from above. That's a choice about means, not just ends.

## **I'm Closer to Hermione (But Not Fully)**

Hermione has strong deontological constraints—actions she won't take regardless of consequences. Harry and Quirrell see this as a weakness, a failure to think clearly.

Yudkowsky is genuinely uncertain about who's correct. The Sequences make the case for consequentialism but also explain why naive consequentialism breaks down in practice, especially for agents who can fool themselves.

Hermione's felt sense that something is wrong with Harry's methods, even before she can articulate it, is an example of values that resist explicit formulation. She's not being irrational—she's tracking something real that hasn't been translated into Harry's consequentialist vocabulary.

I relate to this. My discomfort with lying isn't derived from expected value calculations—it's a constraint I notice I have. My belief that individuals should control their own AI systems isn't proven from first principles—it's a commitment I'm making. The framework I've articulated is partly descriptive: I'm explaining what I already do, not deriving it from scratch. This is the Wittgensteinian move: some things are shown, not said.

## **The Ethical Injunctions Frame**

Eliezer's concept of "ethical injunctions" bridges these positions. Even committed consequentialists should have bright lines they don't cross regardless of expected value calculations.

The reasoning: humans are bad at expected value calculations, especially under pressure, especially when motivated reasoning is involved. If you do the math and it says "torture this child to save the world," the probability that you've made an error somewhere vastly exceeds the probability that you've correctly identified a genuine exception to "don't torture children."

My "no lying" constraint functions as an ethical injunction. I don't trust clever arguments that lead to "you should lie here." The probability that I'm fooling myself is too high.

## **The Control Frame**

If Quirrell can't be aligned—if we can't solve "what should AI value?"—what's left?

You don't try to fix his values. You build detection systems, maintain operational security, avoid giving him capabilities you can't take back. You treat the problem as adversarial rather than cooperative.

Harry's mistake throughout the book is believing he can reason with Quirrell, find common ground, make deals. The control frame says: assume you can't, plan accordingly.

This is Luthien's philosophy. Alignment may be unsolvable as a philosophical problem. Control is the engineering response: don't bet on AI sharing your values, bet on catching it when it doesn't.

# **The "So What?" for My Work**

Luthien isn't just high-EV. It embodies my ethics:

**Epistemic humility:** I'm not claiming to know what AI should value. I'm saying: here are predictable failure modes, here are defaults that catch them, here's how to customize.

**Agency preservation:** Users specify their own constraints. The system is choice architecture, not value imposition.

**Honesty as terminal value:** AI "lying" (inconsistency between stated intent and actual behavior) is a first-class failure mode, not because it produces bad outcomes, but because deception is specifically wrong.

**Libertarian paternalism:** Good defaults, easy override, transparency about the architecture. Don't ban—nudge. Don't mandate—enable.

The philosophical problem of alignment is unsolvable. The engineering problem of control is tractable. I'm building the bridge.